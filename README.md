# Fine-tune GPT-SW3 using quantized LORA

This repository contains minimal code for demonstrating fine-tuning of the GPT-SW3 language models using the QLORA method. The code is written for use with a Nvidia GPU and tested on Windows. The training setup should work with any of the model sizes. With 24 GB of VRAM you can train the 6.7B model with a batch size of 2. To train with less memory you can reduce batch size, enable gradient checkpointing, reduce LORA rank and layers, or simply swap to a smaller base GPT model.

## Setting up Python requirements

Anaconda is a recommended way of installing the required Python packages.
The command below will create an environment with most requirements.

```bash
conda create --name gpt-sw3-finetune python=3.10 pytorch torchvision torchaudio pytorch-cuda=12.1 transformers peft sentencepiece huggingface_hub -c pytorch -c nvidia -c huggingface -c conda-forge
```

Additionally you need "bitsandbytes" which does not have official Windows support. There are however precompiled versions available. Make sure your Anaconda environment is activated and run the following to install bitsandbytes on Windows. This is the latest version as of January 2024. On linux it can be installed with pip.

```bash
conda activate gpt-sw3-finetune
pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl
```

## Dataset (text corpus) for training

This example is written for smaller datasets in a single .txt file.
You can get a ~12 MB sample Swedish dataset containing horror stories from the "creepypodden" podcast by running get_dataset.py. This requires a few more python packages.

```bash
conda install anaconda::beautifulsoup4 anaconda::html5lib
python get_dataset.py
```

## Inference: just generating text

The file "generate_text.py" is meant for testing inference on the finetuned model. Edit the file to set a text generation prompt, generation length or sampling settings. You can compare with the base by setting USE_LORA to True/False. The generated text is saved to "generated_text/output.txt", and each run will be appended to the same file.

## Training

Run "train_gpt-sw3_qlora.py" to train a QLORA adapter model to a base GPT model. All settings can be changed by editing the file.

## Troubleshooting Python environment

New versions of the required Python packages are frequently released so if there are any problems you should follow the installation guides for each of them to check their compatibility.

## Troubleshooting training

Learning rate is a good first step to try increasing/decreasing.

## Output demo

### Prompt:

> Du lyssnar p친 Creepypodden med mig, Jack Werner. I introt h칬rde vi en beskrivning av en varelse som m친nga s칛ger sig ha sett men endast som en forml칬s kontur i dimma och m칬rker. Dagens andra ber칛ttelse kommer ocks친 handla om den. Det 칛r en riktigt kuslig och v칛lskriven historia som kommer fr친n reddits underforum nosleep. Har du sj칛lv n친gonsin g친tt hem sent en kv칛ll n칛r dimman hunnit l칛gga sig, och sedan upplevt hur du ser skepnader som r칬r sig just utanf칬r ditt synf칛lt? Det har i alla fall Elsa, och vi ska f친 h칬ra hennes ber칛ttelse nu, uppl칛st av

### Base model (GPT-SW3-6.7B-V2)

<blockquote>henne sj칛lv! Vars친goda:

Tack f칬r att du gjorde denna pod till verklighet 游뗵

Razmig:
Fantastisk podd! 游때

Kariann:
Helvete vad bra avsnitt! Nu m친ste jag lyssna igenom de gamla igen bara f칬r det..

Masoumeh:
Jag vill tacka er f칬r ett mycket sp칛nnande och underh친llande program. Jag har lyssnat p친 samtliga avsnitt och ni g칬r ett fantastiskt jobb!

Jensine:
Grymt avsnitt! 츿lskar verkligen eran podcast och era historier. Ni verkar vara v칛ldigt sk칬na grabbar! Keep up the good work!

Welday:
H칛rligt att ni gillar det! N칛sta vecka blir det 칛nnu mer l칛skigt! Vi spelar in i morgon f칬rmiddag s친 det borde komma ut n칛sta m친ndag eller tisdag. Hoppas ni inte har n친got emot lite v칛ntan!</blockquote>

### Same model finetuned with QLORA adapter

<blockquote> Rakel Josephson.<br>
Jag vet inte varf칬r jag skriver detta. Jag vet att det finns de som tycker det h칛r 칛r sjukt, men jag kan inte hj칛lpa det. Jag vill bara ber칛tta vad jag s친g.<br>
Ig친r gick jag hem efter jobbet vid halv nio-tiden. N칛r jag kom fram till huset m칛rkte jag att ytterd칬rren stod 칬ppen och att min bil var borta. Jag antog att n친gon hade tagit in bilen eftersom den inte var parkerad d칛r den brukar st친, s친 jag gick in f칬r att leta efter mina nycklar.<br>
N칛r jag letade igenom jackan hittade jag dem inte och t칛nkte att jag kanske gl칬mt dem n친gonstans, s친 jag tittade runt lite i huset utan att hitta dem. Sedan gick jag ut igen och b칬rjade g친 mot bilen. D친 uppt칛ckte jag n친got konstigt i 칬gonvr친n. En m칬rk skugga som r칬rde sig l친ngsamt bakom mig, 친tf칬ljd av ett svagt ljussken. Jag v칛nde mig instinktivt om och fick se att skuggan befann sig precis bakom mig, ungef칛r fem meter bort. Ljuset verkade komma fr친n ett litet f칬nster eller en d칬rr l칛ngre ner l칛ngs v칛gen. Jag f칬rs칬kte vrida huvudet 친t sidan f칬r att se vem det var, men kunde inte r칬ra mig ur fl칛cken - det k칛ndes som om n친gon h칬ll fast mitt huvud. Jag ropade "Hall친?" utan resultat. S친 fort jag 칬ppnade munnen blev ljudet d칛mpat och f칬rsvann helt.</blockquote>
